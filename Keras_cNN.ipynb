{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import keras as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "\n",
    "df_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_map = {l: i for i, l in enumerate(labels)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 40479/40479 [06:27<00:00, 104.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for f, tags in tqdm(df_train.values, miniters=1000):\n",
    "    img = cv2.imread('images\\multi-label/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x_train.append(cv2.resize(img, (64, 64)))\n",
    "    y_train.append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.array(y_train, np.uint8)\n",
    "x_train = np.array(x_train, np.float16) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40479, 64, 64, 3)\n",
      "(40479, 17)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = 35000\n",
    "x_train, x_valid, y_train, y_valid = x_train[:split], x_train[split:], y_train[:split], y_train[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(64, 64, 3),\n",
    "                 padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.36))\n",
    "model.add(Conv2D(256, (3,3), activation='relu'))\n",
    "model.add(Conv2D(512, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(17, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 5479 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6720/35000 [====>.........................] - ETA: 129s - loss: 0.4156 - acc: 0.897 - ETA: 117s - loss: 0.5437 - acc: 0.914 - ETA: 113s - loss: 0.5164 - acc: 0.905 - ETA: 111s - loss: 0.4801 - acc: 0.895 - ETA: 109s - loss: 0.4657 - acc: 0.892 - ETA: 108s - loss: 0.4380 - acc: 0.893 - ETA: 108s - loss: 0.4253 - acc: 0.891 - ETA: 107s - loss: 0.4335 - acc: 0.887 - ETA: 106s - loss: 0.4263 - acc: 0.884 - ETA: 106s - loss: 0.4154 - acc: 0.885 - ETA: 106s - loss: 0.4073 - acc: 0.886 - ETA: 105s - loss: 0.4023 - acc: 0.885 - ETA: 105s - loss: 0.3948 - acc: 0.883 - ETA: 105s - loss: 0.3875 - acc: 0.884 - ETA: 105s - loss: 0.3845 - acc: 0.884 - ETA: 104s - loss: 0.3778 - acc: 0.885 - ETA: 104s - loss: 0.3741 - acc: 0.885 - ETA: 104s - loss: 0.3717 - acc: 0.884 - ETA: 104s - loss: 0.3681 - acc: 0.883 - ETA: 104s - loss: 0.3612 - acc: 0.885 - ETA: 104s - loss: 0.3586 - acc: 0.885 - ETA: 103s - loss: 0.3544 - acc: 0.887 - ETA: 103s - loss: 0.3522 - acc: 0.887 - ETA: 103s - loss: 0.3497 - acc: 0.887 - ETA: 103s - loss: 0.3470 - acc: 0.887 - ETA: 103s - loss: 0.3446 - acc: 0.887 - ETA: 103s - loss: 0.3428 - acc: 0.888 - ETA: 103s - loss: 0.3412 - acc: 0.887 - ETA: 102s - loss: 0.3385 - acc: 0.888 - ETA: 102s - loss: 0.3366 - acc: 0.888 - ETA: 102s - loss: 0.3343 - acc: 0.888 - ETA: 102s - loss: 0.3321 - acc: 0.889 - ETA: 102s - loss: 0.3289 - acc: 0.890 - ETA: 102s - loss: 0.3275 - acc: 0.889 - ETA: 102s - loss: 0.3249 - acc: 0.890 - ETA: 102s - loss: 0.3228 - acc: 0.890 - ETA: 101s - loss: 0.3187 - acc: 0.892 - ETA: 101s - loss: 0.3172 - acc: 0.892 - ETA: 101s - loss: 0.3157 - acc: 0.892 - ETA: 101s - loss: 0.3138 - acc: 0.892 - ETA: 101s - loss: 0.3125 - acc: 0.892 - ETA: 101s - loss: 0.3109 - acc: 0.893 - ETA: 101s - loss: 0.3091 - acc: 0.893 - ETA: 101s - loss: 0.3077 - acc: 0.893 - ETA: 101s - loss: 0.3067 - acc: 0.894 - ETA: 100s - loss: 0.3055 - acc: 0.894 - ETA: 100s - loss: 0.3045 - acc: 0.894 - ETA: 100s - loss: 0.3030 - acc: 0.894 - ETA: 100s - loss: 0.3021 - acc: 0.894 - ETA: 100s - loss: 0.3007 - acc: 0.894 - ETA: 100s - loss: 0.2995 - acc: 0.894 - ETA: 100s - loss: 0.2984 - acc: 0.895 - ETA: 100s - loss: 0.2975 - acc: 0.895 - ETA: 100s - loss: 0.2960 - acc: 0.895 - ETA: 99s - loss: 0.2946 - acc: 0.896 - ETA: 99s - loss: 0.2941 - acc: 0.89 - ETA: 99s - loss: 0.2938 - acc: 0.89 - ETA: 99s - loss: 0.2930 - acc: 0.89 - ETA: 99s - loss: 0.2924 - acc: 0.89 - ETA: 99s - loss: 0.2923 - acc: 0.89 - ETA: 99s - loss: 0.2911 - acc: 0.89 - ETA: 99s - loss: 0.2905 - acc: 0.89 - ETA: 99s - loss: 0.2902 - acc: 0.89 - ETA: 99s - loss: 0.2885 - acc: 0.89 - ETA: 98s - loss: 0.2876 - acc: 0.89 - ETA: 98s - loss: 0.2867 - acc: 0.89 - ETA: 98s - loss: 0.2851 - acc: 0.89 - ETA: 98s - loss: 0.2840 - acc: 0.89 - ETA: 98s - loss: 0.2841 - acc: 0.89 - ETA: 98s - loss: 0.2837 - acc: 0.89 - ETA: 98s - loss: 0.2828 - acc: 0.89 - ETA: 98s - loss: 0.2821 - acc: 0.89 - ETA: 98s - loss: 0.2818 - acc: 0.89 - ETA: 98s - loss: 0.2811 - acc: 0.89 - ETA: 97s - loss: 0.2808 - acc: 0.89 - ETA: 97s - loss: 0.2805 - acc: 0.89 - ETA: 97s - loss: 0.2797 - acc: 0.89 - ETA: 97s - loss: 0.2787 - acc: 0.89 - ETA: 97s - loss: 0.2776 - acc: 0.90 - ETA: 97s - loss: 0.2766 - acc: 0.90 - ETA: 97s - loss: 0.2774 - acc: 0.90 - ETA: 97s - loss: 0.2772 - acc: 0.90 - ETA: 97s - loss: 0.2767 - acc: 0.90 - ETA: 97s - loss: 0.2764 - acc: 0.90 - ETA: 97s - loss: 0.2762 - acc: 0.90 - ETA: 96s - loss: 0.2756 - acc: 0.90 - ETA: 96s - loss: 0.2755 - acc: 0.90 - ETA: 96s - loss: 0.2755 - acc: 0.90 - ETA: 96s - loss: 0.2744 - acc: 0.90 - ETA: 96s - loss: 0.2746 - acc: 0.90 - ETA: 96s - loss: 0.2745 - acc: 0.90 - ETA: 96s - loss: 0.2737 - acc: 0.90 - ETA: 96s - loss: 0.2730 - acc: 0.90 - ETA: 96s - loss: 0.2727 - acc: 0.90 - ETA: 96s - loss: 0.2725 - acc: 0.90 - ETA: 96s - loss: 0.2725 - acc: 0.90 - ETA: 95s - loss: 0.2722 - acc: 0.90 - ETA: 95s - loss: 0.2718 - acc: 0.90 - ETA: 95s - loss: 0.2713 - acc: 0.90 - ETA: 95s - loss: 0.2709 - acc: 0.90 - ETA: 95s - loss: 0.2703 - acc: 0.90 - ETA: 95s - loss: 0.2702 - acc: 0.90 - ETA: 95s - loss: 0.2700 - acc: 0.90 - ETA: 95s - loss: 0.2696 - acc: 0.90 - ETA: 95s - loss: 0.2695 - acc: 0.90 - ETA: 95s - loss: 0.2693 - acc: 0.90 - ETA: 94s - loss: 0.2694 - acc: 0.90 - ETA: 94s - loss: 0.2687 - acc: 0.90 - ETA: 94s - loss: 0.2684 - acc: 0.90 - ETA: 94s - loss: 0.2682 - acc: 0.90 - ETA: 94s - loss: 0.2675 - acc: 0.90 - ETA: 94s - loss: 0.2672 - acc: 0.90 - ETA: 94s - loss: 0.2673 - acc: 0.90 - ETA: 94s - loss: 0.2671 - acc: 0.90 - ETA: 94s - loss: 0.2670 - acc: 0.90 - ETA: 94s - loss: 0.2669 - acc: 0.90 - ETA: 93s - loss: 0.2667 - acc: 0.90 - ETA: 93s - loss: 0.2661 - acc: 0.90 - ETA: 93s - loss: 0.2658 - acc: 0.90 - ETA: 93s - loss: 0.2655 - acc: 0.90 - ETA: 93s - loss: 0.2654 - acc: 0.90 - ETA: 93s - loss: 0.2649 - acc: 0.90 - ETA: 93s - loss: 0.2646 - acc: 0.90 - ETA: 93s - loss: 0.2646 - acc: 0.90 - ETA: 93s - loss: 0.2651 - acc: 0.90 - ETA: 93s - loss: 0.2647 - acc: 0.90 - ETA: 92s - loss: 0.2644 - acc: 0.90 - ETA: 92s - loss: 0.2642 - acc: 0.90 - ETA: 92s - loss: 0.2640 - acc: 0.90 - ETA: 92s - loss: 0.2637 - acc: 0.90 - ETA: 92s - loss: 0.2635 - acc: 0.90 - ETA: 92s - loss: 0.2637 - acc: 0.90 - ETA: 92s - loss: 0.2637 - acc: 0.90 - ETA: 92s - loss: 0.2633 - acc: 0.90 - ETA: 92s - loss: 0.2628 - acc: 0.90 - ETA: 92s - loss: 0.2626 - acc: 0.90 - ETA: 92s - loss: 0.2624 - acc: 0.90 - ETA: 91s - loss: 0.2623 - acc: 0.90 - ETA: 91s - loss: 0.2623 - acc: 0.90 - ETA: 91s - loss: 0.2619 - acc: 0.90 - ETA: 91s - loss: 0.2618 - acc: 0.90 - ETA: 91s - loss: 0.2616 - acc: 0.90 - ETA: 91s - loss: 0.2616 - acc: 0.90 - ETA: 91s - loss: 0.2613 - acc: 0.90 - ETA: 91s - loss: 0.2612 - acc: 0.90 - ETA: 91s - loss: 0.2612 - acc: 0.90 - ETA: 91s - loss: 0.2609 - acc: 0.90 - ETA: 90s - loss: 0.2608 - acc: 0.90 - ETA: 90s - loss: 0.2606 - acc: 0.90 - ETA: 90s - loss: 0.2604 - acc: 0.90 - ETA: 90s - loss: 0.2605 - acc: 0.90 - ETA: 90s - loss: 0.2604 - acc: 0.90 - ETA: 90s - loss: 0.2603 - acc: 0.90 - ETA: 90s - loss: 0.2597 - acc: 0.90 - ETA: 90s - loss: 0.2597 - acc: 0.90 - ETA: 90s - loss: 0.2592 - acc: 0.90 - ETA: 90s - loss: 0.2592 - acc: 0.90 - ETA: 89s - loss: 0.2589 - acc: 0.90 - ETA: 89s - loss: 0.2586 - acc: 0.90 - ETA: 89s - loss: 0.2586 - acc: 0.90 - ETA: 89s - loss: 0.2585 - acc: 0.90 - ETA: 89s - loss: 0.2581 - acc: 0.90 - ETA: 89s - loss: 0.2579 - acc: 0.90 - ETA: 89s - loss: 0.2576 - acc: 0.90 - ETA: 89s - loss: 0.2573 - acc: 0.90 - ETA: 89s - loss: 0.2573 - acc: 0.90 - ETA: 89s - loss: 0.2571 - acc: 0.90 - ETA: 89s - loss: 0.2570 - acc: 0.90 - ETA: 88s - loss: 0.2569 - acc: 0.90 - ETA: 88s - loss: 0.2570 - acc: 0.90 - ETA: 88s - loss: 0.2570 - acc: 0.90 - ETA: 88s - loss: 0.2569 - acc: 0.90 - ETA: 88s - loss: 0.2568 - acc: 0.90 - ETA: 88s - loss: 0.2565 - acc: 0.90 - ETA: 88s - loss: 0.2563 - acc: 0.90 - ETA: 88s - loss: 0.2563 - acc: 0.90 - ETA: 88s - loss: 0.2562 - acc: 0.90 - ETA: 88s - loss: 0.2563 - acc: 0.90 - ETA: 87s - loss: 0.2558 - acc: 0.90 - ETA: 87s - loss: 0.2555 - acc: 0.90 - ETA: 87s - loss: 0.2554 - acc: 0.90 - ETA: 87s - loss: 0.2555 - acc: 0.90 - ETA: 87s - loss: 0.2552 - acc: 0.90 - ETA: 87s - loss: 0.2551 - acc: 0.90 - ETA: 87s - loss: 0.2550 - acc: 0.90 - ETA: 87s - loss: 0.2549 - acc: 0.90 - ETA: 87s - loss: 0.2547 - acc: 0.90 - ETA: 87s - loss: 0.2544 - acc: 0.90 - ETA: 87s - loss: 0.2543 - acc: 0.90 - ETA: 86s - loss: 0.2540 - acc: 0.90 - ETA: 86s - loss: 0.2536 - acc: 0.90 - ETA: 86s - loss: 0.2535 - acc: 0.90 - ETA: 86s - loss: 0.2536 - acc: 0.90 - ETA: 86s - loss: 0.2534 - acc: 0.90 - ETA: 86s - loss: 0.2534 - acc: 0.90 - ETA: 86s - loss: 0.2533 - acc: 0.90 - ETA: 86s - loss: 0.2532 - acc: 0.90 - ETA: 86s - loss: 0.2530 - acc: 0.90 - ETA: 86s - loss: 0.2528 - acc: 0.90 - ETA: 85s - loss: 0.2526 - acc: 0.90 - ETA: 85s - loss: 0.2526 - acc: 0.90 - ETA: 85s - loss: 0.2527 - acc: 0.90 - ETA: 85s - loss: 0.2525 - acc: 0.90 - ETA: 85s - loss: 0.2525 - acc: 0.90 - ETA: 85s - loss: 0.2523 - acc: 0.90 - ETA: 85s - loss: 0.2522 - acc: 0.90 - ETA: 85s - loss: 0.2523 - acc: 0.90 - ETA: 85s - loss: 0.2522 - acc: 0.90 - ETA: 85s - loss: 0.2520 - acc: 0.90 - ETA: 85s - loss: 0.2518 - acc: 0.9047\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13600/35000 [==========>...................] - ETA: 84s - loss: 0.2516 - acc: 0.90 - ETA: 84s - loss: 0.2514 - acc: 0.90 - ETA: 84s - loss: 0.2512 - acc: 0.90 - ETA: 84s - loss: 0.2509 - acc: 0.90 - ETA: 84s - loss: 0.2506 - acc: 0.90 - ETA: 84s - loss: 0.2506 - acc: 0.90 - ETA: 84s - loss: 0.2505 - acc: 0.90 - ETA: 84s - loss: 0.2504 - acc: 0.90 - ETA: 84s - loss: 0.2503 - acc: 0.90 - ETA: 84s - loss: 0.2499 - acc: 0.90 - ETA: 83s - loss: 0.2498 - acc: 0.90 - ETA: 83s - loss: 0.2498 - acc: 0.90 - ETA: 83s - loss: 0.2496 - acc: 0.90 - ETA: 83s - loss: 0.2497 - acc: 0.90 - ETA: 83s - loss: 0.2495 - acc: 0.90 - ETA: 83s - loss: 0.2494 - acc: 0.90 - ETA: 83s - loss: 0.2491 - acc: 0.90 - ETA: 83s - loss: 0.2490 - acc: 0.90 - ETA: 83s - loss: 0.2489 - acc: 0.90 - ETA: 83s - loss: 0.2487 - acc: 0.90 - ETA: 83s - loss: 0.2487 - acc: 0.90 - ETA: 82s - loss: 0.2487 - acc: 0.90 - ETA: 82s - loss: 0.2487 - acc: 0.90 - ETA: 82s - loss: 0.2486 - acc: 0.90 - ETA: 82s - loss: 0.2488 - acc: 0.90 - ETA: 82s - loss: 0.2486 - acc: 0.90 - ETA: 82s - loss: 0.2485 - acc: 0.90 - ETA: 82s - loss: 0.2483 - acc: 0.90 - ETA: 82s - loss: 0.2481 - acc: 0.90 - ETA: 82s - loss: 0.2478 - acc: 0.90 - ETA: 82s - loss: 0.2477 - acc: 0.90 - ETA: 82s - loss: 0.2478 - acc: 0.90 - ETA: 81s - loss: 0.2477 - acc: 0.90 - ETA: 81s - loss: 0.2478 - acc: 0.90 - ETA: 81s - loss: 0.2476 - acc: 0.90 - ETA: 81s - loss: 0.2475 - acc: 0.90 - ETA: 81s - loss: 0.2474 - acc: 0.90 - ETA: 81s - loss: 0.2474 - acc: 0.90 - ETA: 81s - loss: 0.2473 - acc: 0.90 - ETA: 81s - loss: 0.2471 - acc: 0.90 - ETA: 81s - loss: 0.2471 - acc: 0.90 - ETA: 81s - loss: 0.2472 - acc: 0.90 - ETA: 80s - loss: 0.2470 - acc: 0.90 - ETA: 80s - loss: 0.2467 - acc: 0.90 - ETA: 80s - loss: 0.2464 - acc: 0.90 - ETA: 80s - loss: 0.2464 - acc: 0.90 - ETA: 80s - loss: 0.2465 - acc: 0.90 - ETA: 80s - loss: 0.2463 - acc: 0.90 - ETA: 80s - loss: 0.2463 - acc: 0.90 - ETA: 80s - loss: 0.2462 - acc: 0.90 - ETA: 80s - loss: 0.2463 - acc: 0.90 - ETA: 80s - loss: 0.2464 - acc: 0.90 - ETA: 79s - loss: 0.2463 - acc: 0.90 - ETA: 79s - loss: 0.2463 - acc: 0.90 - ETA: 79s - loss: 0.2463 - acc: 0.90 - ETA: 79s - loss: 0.2466 - acc: 0.90 - ETA: 79s - loss: 0.2465 - acc: 0.90 - ETA: 79s - loss: 0.2468 - acc: 0.90 - ETA: 79s - loss: 0.2467 - acc: 0.90 - ETA: 79s - loss: 0.2467 - acc: 0.90 - ETA: 79s - loss: 0.2467 - acc: 0.90 - ETA: 79s - loss: 0.2467 - acc: 0.90 - ETA: 78s - loss: 0.2466 - acc: 0.90 - ETA: 78s - loss: 0.2465 - acc: 0.90 - ETA: 78s - loss: 0.2466 - acc: 0.90 - ETA: 78s - loss: 0.2466 - acc: 0.90 - ETA: 78s - loss: 0.2466 - acc: 0.90 - ETA: 78s - loss: 0.2464 - acc: 0.90 - ETA: 78s - loss: 0.2463 - acc: 0.90 - ETA: 78s - loss: 0.2461 - acc: 0.90 - ETA: 78s - loss: 0.2460 - acc: 0.90 - ETA: 78s - loss: 0.2459 - acc: 0.90 - ETA: 78s - loss: 0.2459 - acc: 0.90 - ETA: 77s - loss: 0.2459 - acc: 0.90 - ETA: 77s - loss: 0.2458 - acc: 0.90 - ETA: 77s - loss: 0.2459 - acc: 0.90 - ETA: 77s - loss: 0.2458 - acc: 0.90 - ETA: 77s - loss: 0.2458 - acc: 0.90 - ETA: 77s - loss: 0.2458 - acc: 0.90 - ETA: 77s - loss: 0.2458 - acc: 0.90 - ETA: 77s - loss: 0.2459 - acc: 0.90 - ETA: 77s - loss: 0.2458 - acc: 0.90 - ETA: 77s - loss: 0.2458 - acc: 0.90 - ETA: 76s - loss: 0.2456 - acc: 0.90 - ETA: 76s - loss: 0.2455 - acc: 0.90 - ETA: 76s - loss: 0.2453 - acc: 0.90 - ETA: 76s - loss: 0.2452 - acc: 0.90 - ETA: 76s - loss: 0.2451 - acc: 0.90 - ETA: 76s - loss: 0.2451 - acc: 0.90 - ETA: 76s - loss: 0.2450 - acc: 0.90 - ETA: 76s - loss: 0.2450 - acc: 0.90 - ETA: 76s - loss: 0.2449 - acc: 0.90 - ETA: 76s - loss: 0.2450 - acc: 0.90 - ETA: 75s - loss: 0.2449 - acc: 0.90 - ETA: 75s - loss: 0.2448 - acc: 0.90 - ETA: 75s - loss: 0.2447 - acc: 0.90 - ETA: 75s - loss: 0.2446 - acc: 0.90 - ETA: 75s - loss: 0.2446 - acc: 0.90 - ETA: 75s - loss: 0.2446 - acc: 0.90 - ETA: 75s - loss: 0.2444 - acc: 0.90 - ETA: 75s - loss: 0.2442 - acc: 0.90 - ETA: 75s - loss: 0.2442 - acc: 0.90 - ETA: 75s - loss: 0.2442 - acc: 0.90 - ETA: 75s - loss: 0.2441 - acc: 0.90 - ETA: 74s - loss: 0.2439 - acc: 0.90 - ETA: 74s - loss: 0.2439 - acc: 0.90 - ETA: 74s - loss: 0.2439 - acc: 0.90 - ETA: 74s - loss: 0.2438 - acc: 0.90 - ETA: 74s - loss: 0.2436 - acc: 0.90 - ETA: 74s - loss: 0.2436 - acc: 0.90 - ETA: 74s - loss: 0.2436 - acc: 0.90 - ETA: 74s - loss: 0.2434 - acc: 0.90 - ETA: 74s - loss: 0.2432 - acc: 0.90 - ETA: 74s - loss: 0.2431 - acc: 0.90 - ETA: 73s - loss: 0.2430 - acc: 0.90 - ETA: 73s - loss: 0.2430 - acc: 0.90 - ETA: 73s - loss: 0.2429 - acc: 0.90 - ETA: 73s - loss: 0.2428 - acc: 0.90 - ETA: 73s - loss: 0.2427 - acc: 0.90 - ETA: 73s - loss: 0.2427 - acc: 0.90 - ETA: 73s - loss: 0.2427 - acc: 0.90 - ETA: 73s - loss: 0.2427 - acc: 0.90 - ETA: 73s - loss: 0.2426 - acc: 0.90 - ETA: 73s - loss: 0.2425 - acc: 0.90 - ETA: 72s - loss: 0.2425 - acc: 0.90 - ETA: 72s - loss: 0.2424 - acc: 0.90 - ETA: 72s - loss: 0.2424 - acc: 0.90 - ETA: 72s - loss: 0.2424 - acc: 0.90 - ETA: 72s - loss: 0.2423 - acc: 0.90 - ETA: 72s - loss: 0.2422 - acc: 0.90 - ETA: 72s - loss: 0.2421 - acc: 0.90 - ETA: 72s - loss: 0.2420 - acc: 0.90 - ETA: 72s - loss: 0.2422 - acc: 0.90 - ETA: 72s - loss: 0.2421 - acc: 0.90 - ETA: 72s - loss: 0.2418 - acc: 0.90 - ETA: 71s - loss: 0.2417 - acc: 0.90 - ETA: 71s - loss: 0.2415 - acc: 0.90 - ETA: 71s - loss: 0.2415 - acc: 0.90 - ETA: 71s - loss: 0.2414 - acc: 0.90 - ETA: 71s - loss: 0.2412 - acc: 0.90 - ETA: 71s - loss: 0.2411 - acc: 0.90 - ETA: 71s - loss: 0.2410 - acc: 0.90 - ETA: 71s - loss: 0.2411 - acc: 0.90 - ETA: 71s - loss: 0.2410 - acc: 0.90 - ETA: 71s - loss: 0.2409 - acc: 0.90 - ETA: 70s - loss: 0.2408 - acc: 0.90 - ETA: 70s - loss: 0.2406 - acc: 0.90 - ETA: 70s - loss: 0.2406 - acc: 0.90 - ETA: 70s - loss: 0.2404 - acc: 0.90 - ETA: 70s - loss: 0.2402 - acc: 0.90 - ETA: 70s - loss: 0.2401 - acc: 0.90 - ETA: 70s - loss: 0.2400 - acc: 0.90 - ETA: 70s - loss: 0.2400 - acc: 0.90 - ETA: 70s - loss: 0.2399 - acc: 0.90 - ETA: 70s - loss: 0.2400 - acc: 0.90 - ETA: 70s - loss: 0.2401 - acc: 0.90 - ETA: 69s - loss: 0.2399 - acc: 0.90 - ETA: 69s - loss: 0.2398 - acc: 0.90 - ETA: 69s - loss: 0.2397 - acc: 0.90 - ETA: 69s - loss: 0.2396 - acc: 0.90 - ETA: 69s - loss: 0.2394 - acc: 0.90 - ETA: 69s - loss: 0.2395 - acc: 0.90 - ETA: 69s - loss: 0.2394 - acc: 0.90 - ETA: 69s - loss: 0.2392 - acc: 0.90 - ETA: 69s - loss: 0.2392 - acc: 0.90 - ETA: 69s - loss: 0.2393 - acc: 0.90 - ETA: 68s - loss: 0.2395 - acc: 0.90 - ETA: 68s - loss: 0.2394 - acc: 0.90 - ETA: 68s - loss: 0.2392 - acc: 0.90 - ETA: 68s - loss: 0.2391 - acc: 0.90 - ETA: 68s - loss: 0.2391 - acc: 0.90 - ETA: 68s - loss: 0.2391 - acc: 0.90 - ETA: 68s - loss: 0.2391 - acc: 0.90 - ETA: 68s - loss: 0.2390 - acc: 0.90 - ETA: 68s - loss: 0.2389 - acc: 0.90 - ETA: 68s - loss: 0.2390 - acc: 0.90 - ETA: 67s - loss: 0.2390 - acc: 0.90 - ETA: 67s - loss: 0.2390 - acc: 0.90 - ETA: 67s - loss: 0.2390 - acc: 0.90 - ETA: 67s - loss: 0.2389 - acc: 0.90 - ETA: 67s - loss: 0.2389 - acc: 0.90 - ETA: 67s - loss: 0.2390 - acc: 0.90 - ETA: 67s - loss: 0.2388 - acc: 0.90 - ETA: 67s - loss: 0.2387 - acc: 0.90 - ETA: 67s - loss: 0.2386 - acc: 0.90 - ETA: 67s - loss: 0.2386 - acc: 0.90 - ETA: 67s - loss: 0.2385 - acc: 0.90 - ETA: 66s - loss: 0.2386 - acc: 0.90 - ETA: 66s - loss: 0.2386 - acc: 0.90 - ETA: 66s - loss: 0.2386 - acc: 0.90 - ETA: 66s - loss: 0.2387 - acc: 0.90 - ETA: 66s - loss: 0.2386 - acc: 0.90 - ETA: 66s - loss: 0.2385 - acc: 0.90 - ETA: 66s - loss: 0.2383 - acc: 0.90 - ETA: 66s - loss: 0.2383 - acc: 0.90 - ETA: 66s - loss: 0.2382 - acc: 0.90 - ETA: 66s - loss: 0.2381 - acc: 0.90 - ETA: 65s - loss: 0.2381 - acc: 0.90 - ETA: 65s - loss: 0.2381 - acc: 0.90 - ETA: 65s - loss: 0.2381 - acc: 0.90 - ETA: 65s - loss: 0.2380 - acc: 0.90 - ETA: 65s - loss: 0.2380 - acc: 0.90 - ETA: 65s - loss: 0.2379 - acc: 0.90 - ETA: 65s - loss: 0.2380 - acc: 0.90 - ETA: 65s - loss: 0.2379 - acc: 0.90 - ETA: 65s - loss: 0.2379 - acc: 0.90 - ETA: 65s - loss: 0.2378 - acc: 0.90 - ETA: 65s - loss: 0.2377 - acc: 0.90 - ETA: 64s - loss: 0.2377 - acc: 0.90 - ETA: 64s - loss: 0.2376 - acc: 0.90 - ETA: 64s - loss: 0.2375 - acc: 0.90 - ETA: 64s - loss: 0.2375 - acc: 0.90 - ETA: 64s - loss: 0.2375 - acc: 0.90 - ETA: 64s - loss: 0.2375 - acc: 0.90 - ETA: 64s - loss: 0.2374 - acc: 0.9089"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18912/35000 [===============>..............] - ETA: 64s - loss: 0.2374 - acc: 0.90 - ETA: 64s - loss: 0.2373 - acc: 0.90 - ETA: 64s - loss: 0.2372 - acc: 0.90 - ETA: 63s - loss: 0.2371 - acc: 0.90 - ETA: 63s - loss: 0.2370 - acc: 0.90 - ETA: 63s - loss: 0.2369 - acc: 0.90 - ETA: 63s - loss: 0.2369 - acc: 0.90 - ETA: 63s - loss: 0.2369 - acc: 0.90 - ETA: 63s - loss: 0.2368 - acc: 0.90 - ETA: 63s - loss: 0.2367 - acc: 0.90 - ETA: 63s - loss: 0.2367 - acc: 0.90 - ETA: 63s - loss: 0.2366 - acc: 0.90 - ETA: 63s - loss: 0.2366 - acc: 0.90 - ETA: 62s - loss: 0.2364 - acc: 0.90 - ETA: 62s - loss: 0.2364 - acc: 0.90 - ETA: 62s - loss: 0.2362 - acc: 0.90 - ETA: 62s - loss: 0.2361 - acc: 0.90 - ETA: 62s - loss: 0.2362 - acc: 0.90 - ETA: 62s - loss: 0.2362 - acc: 0.90 - ETA: 62s - loss: 0.2362 - acc: 0.90 - ETA: 62s - loss: 0.2361 - acc: 0.90 - ETA: 62s - loss: 0.2361 - acc: 0.90 - ETA: 62s - loss: 0.2360 - acc: 0.90 - ETA: 62s - loss: 0.2359 - acc: 0.90 - ETA: 61s - loss: 0.2358 - acc: 0.90 - ETA: 61s - loss: 0.2357 - acc: 0.90 - ETA: 61s - loss: 0.2356 - acc: 0.90 - ETA: 61s - loss: 0.2355 - acc: 0.90 - ETA: 61s - loss: 0.2355 - acc: 0.90 - ETA: 61s - loss: 0.2353 - acc: 0.90 - ETA: 61s - loss: 0.2353 - acc: 0.90 - ETA: 61s - loss: 0.2352 - acc: 0.90 - ETA: 61s - loss: 0.2351 - acc: 0.90 - ETA: 61s - loss: 0.2351 - acc: 0.90 - ETA: 60s - loss: 0.2349 - acc: 0.90 - ETA: 60s - loss: 0.2349 - acc: 0.90 - ETA: 60s - loss: 0.2347 - acc: 0.90 - ETA: 60s - loss: 0.2347 - acc: 0.90 - ETA: 60s - loss: 0.2346 - acc: 0.91 - ETA: 60s - loss: 0.2345 - acc: 0.91 - ETA: 60s - loss: 0.2344 - acc: 0.91 - ETA: 60s - loss: 0.2344 - acc: 0.91 - ETA: 60s - loss: 0.2345 - acc: 0.91 - ETA: 60s - loss: 0.2344 - acc: 0.91 - ETA: 60s - loss: 0.2345 - acc: 0.90 - ETA: 59s - loss: 0.2344 - acc: 0.91 - ETA: 59s - loss: 0.2343 - acc: 0.91 - ETA: 59s - loss: 0.2343 - acc: 0.91 - ETA: 59s - loss: 0.2343 - acc: 0.91 - ETA: 59s - loss: 0.2342 - acc: 0.91 - ETA: 59s - loss: 0.2341 - acc: 0.91 - ETA: 59s - loss: 0.2340 - acc: 0.91 - ETA: 59s - loss: 0.2340 - acc: 0.91 - ETA: 59s - loss: 0.2339 - acc: 0.91 - ETA: 59s - loss: 0.2338 - acc: 0.91 - ETA: 58s - loss: 0.2338 - acc: 0.91 - ETA: 58s - loss: 0.2337 - acc: 0.91 - ETA: 58s - loss: 0.2336 - acc: 0.91 - ETA: 58s - loss: 0.2336 - acc: 0.91 - ETA: 58s - loss: 0.2335 - acc: 0.91 - ETA: 58s - loss: 0.2335 - acc: 0.91 - ETA: 58s - loss: 0.2335 - acc: 0.91 - ETA: 58s - loss: 0.2334 - acc: 0.91 - ETA: 58s - loss: 0.2333 - acc: 0.91 - ETA: 58s - loss: 0.2333 - acc: 0.91 - ETA: 57s - loss: 0.2332 - acc: 0.91 - ETA: 57s - loss: 0.2332 - acc: 0.91 - ETA: 57s - loss: 0.2331 - acc: 0.91 - ETA: 57s - loss: 0.2331 - acc: 0.91 - ETA: 57s - loss: 0.2330 - acc: 0.91 - ETA: 57s - loss: 0.2330 - acc: 0.91 - ETA: 57s - loss: 0.2330 - acc: 0.91 - ETA: 57s - loss: 0.2328 - acc: 0.91 - ETA: 57s - loss: 0.2327 - acc: 0.91 - ETA: 57s - loss: 0.2326 - acc: 0.91 - ETA: 57s - loss: 0.2326 - acc: 0.91 - ETA: 56s - loss: 0.2325 - acc: 0.91 - ETA: 56s - loss: 0.2325 - acc: 0.91 - ETA: 56s - loss: 0.2326 - acc: 0.91 - ETA: 56s - loss: 0.2326 - acc: 0.91 - ETA: 56s - loss: 0.2325 - acc: 0.91 - ETA: 56s - loss: 0.2325 - acc: 0.91 - ETA: 56s - loss: 0.2325 - acc: 0.91 - ETA: 56s - loss: 0.2324 - acc: 0.91 - ETA: 56s - loss: 0.2325 - acc: 0.91 - ETA: 56s - loss: 0.2324 - acc: 0.91 - ETA: 55s - loss: 0.2324 - acc: 0.91 - ETA: 55s - loss: 0.2323 - acc: 0.91 - ETA: 55s - loss: 0.2324 - acc: 0.91 - ETA: 55s - loss: 0.2324 - acc: 0.91 - ETA: 55s - loss: 0.2323 - acc: 0.91 - ETA: 55s - loss: 0.2323 - acc: 0.91 - ETA: 55s - loss: 0.2323 - acc: 0.91 - ETA: 55s - loss: 0.2323 - acc: 0.91 - ETA: 55s - loss: 0.2324 - acc: 0.91 - ETA: 55s - loss: 0.2323 - acc: 0.91 - ETA: 54s - loss: 0.2323 - acc: 0.91 - ETA: 54s - loss: 0.2322 - acc: 0.91 - ETA: 54s - loss: 0.2322 - acc: 0.91 - ETA: 54s - loss: 0.2321 - acc: 0.91 - ETA: 54s - loss: 0.2320 - acc: 0.91 - ETA: 54s - loss: 0.2320 - acc: 0.91 - ETA: 54s - loss: 0.2319 - acc: 0.91 - ETA: 54s - loss: 0.2318 - acc: 0.91 - ETA: 54s - loss: 0.2317 - acc: 0.91 - ETA: 54s - loss: 0.2316 - acc: 0.91 - ETA: 54s - loss: 0.2317 - acc: 0.91 - ETA: 53s - loss: 0.2315 - acc: 0.91 - ETA: 53s - loss: 0.2316 - acc: 0.91 - ETA: 53s - loss: 0.2315 - acc: 0.91 - ETA: 53s - loss: 0.2314 - acc: 0.91 - ETA: 53s - loss: 0.2313 - acc: 0.91 - ETA: 53s - loss: 0.2312 - acc: 0.91 - ETA: 53s - loss: 0.2311 - acc: 0.91 - ETA: 53s - loss: 0.2311 - acc: 0.91 - ETA: 53s - loss: 0.2311 - acc: 0.91 - ETA: 53s - loss: 0.2310 - acc: 0.91 - ETA: 52s - loss: 0.2309 - acc: 0.91 - ETA: 52s - loss: 0.2309 - acc: 0.91 - ETA: 52s - loss: 0.2308 - acc: 0.91 - ETA: 52s - loss: 0.2308 - acc: 0.91 - ETA: 52s - loss: 0.2306 - acc: 0.91 - ETA: 52s - loss: 0.2306 - acc: 0.91 - ETA: 52s - loss: 0.2306 - acc: 0.91 - ETA: 52s - loss: 0.2305 - acc: 0.91 - ETA: 52s - loss: 0.2305 - acc: 0.91 - ETA: 52s - loss: 0.2304 - acc: 0.91 - ETA: 52s - loss: 0.2304 - acc: 0.91 - ETA: 51s - loss: 0.2304 - acc: 0.91 - ETA: 51s - loss: 0.2304 - acc: 0.91 - ETA: 51s - loss: 0.2304 - acc: 0.91 - ETA: 51s - loss: 0.2303 - acc: 0.91 - ETA: 51s - loss: 0.2302 - acc: 0.91 - ETA: 51s - loss: 0.2302 - acc: 0.91 - ETA: 51s - loss: 0.2302 - acc: 0.91 - ETA: 51s - loss: 0.2302 - acc: 0.91 - ETA: 51s - loss: 0.2301 - acc: 0.91 - ETA: 51s - loss: 0.2301 - acc: 0.91 - ETA: 50s - loss: 0.2300 - acc: 0.91 - ETA: 50s - loss: 0.2299 - acc: 0.91 - ETA: 50s - loss: 0.2298 - acc: 0.91 - ETA: 50s - loss: 0.2297 - acc: 0.91 - ETA: 50s - loss: 0.2298 - acc: 0.91 - ETA: 50s - loss: 0.2297 - acc: 0.91 - ETA: 50s - loss: 0.2296 - acc: 0.91 - ETA: 50s - loss: 0.2296 - acc: 0.91 - ETA: 50s - loss: 0.2295 - acc: 0.91 - ETA: 50s - loss: 0.2295 - acc: 0.91 - ETA: 49s - loss: 0.2294 - acc: 0.91 - ETA: 49s - loss: 0.2293 - acc: 0.91 - ETA: 49s - loss: 0.2293 - acc: 0.91 - ETA: 49s - loss: 0.2293 - acc: 0.91 - ETA: 49s - loss: 0.2293 - acc: 0.91 - ETA: 49s - loss: 0.2292 - acc: 0.91 - ETA: 49s - loss: 0.2291 - acc: 0.91 - ETA: 49s - loss: 0.2290 - acc: 0.91 - ETA: 49s - loss: 0.2289 - acc: 0.91 - ETA: 49s - loss: 0.2288 - acc: 0.91 - ETA: 49s - loss: 0.2288 - acc: 0.91 - ETA: 48s - loss: 0.2287 - acc: 0.91 - ETA: 48s - loss: 0.2287 - acc: 0.91 - ETA: 48s - loss: 0.2287 - acc: 0.91 - ETA: 48s - loss: 0.2287 - acc: 0.91 - ETA: 48s - loss: 0.2286 - acc: 0.91 - ETA: 48s - loss: 0.2285 - acc: 0.91 - ETA: 48s - loss: 0.2286 - acc: 0.9123"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-0a3b970bd504>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m           validation_data=(x_valid, y_valid))\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Romtean\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\Romtean\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Romtean\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Romtean\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2229\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Romtean\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Romtean\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Romtean\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\Romtean\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Romtean\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          verbose=1,\n",
    "          validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 1 0 0]]\n",
      "[[  3.41795478e-03   3.49149406e-01   6.88070012e-03 ...,   4.70918044e-02\n",
      "    1.05543230e-02   2.20186293e-01]\n",
      " [  3.62914783e-04   5.43621778e-01   2.14697188e-03 ...,   6.67340914e-03\n",
      "    5.51735796e-03   2.05540493e-01]\n",
      " [  6.81488527e-05   5.49709022e-01   1.44418282e-03 ...,   1.35168675e-02\n",
      "    3.66808847e-03   3.67286116e-01]\n",
      " ..., \n",
      " [  8.01305578e-04   5.55778325e-01   8.93370307e-04 ...,   4.28759269e-02\n",
      "    2.45495653e-03   2.20832571e-01]\n",
      " [  3.67341359e-04   5.96242368e-01   4.87625774e-04 ...,   5.86157180e-02\n",
      "    1.51333096e-03   2.10207671e-01]\n",
      " [  7.93422514e-04   2.16321394e-01   1.50585244e-03 ...,   9.01548147e-01\n",
      "    1.61375955e-03   2.62208879e-01]]\n",
      "0.842427989782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "p_valid = model.predict(x_valid, batch_size=128)\n",
    "print(y_valid)\n",
    "print(p_valid)\n",
    "print(fbeta_score(y_valid, np.array(p_valid) > 0.25, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
